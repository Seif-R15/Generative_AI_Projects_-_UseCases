{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seif-R15/Generative_AI_Projects_-_UseCases/blob/main/FLAN-t5_Fine_Tuning_PEFT_%26_RAG/FLAN_t5_Fine_Tuning_PEFT_%26_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ead32b-d4ab-431e-b64d-7a59b8962fae",
      "metadata": {
        "id": "43ead32b-d4ab-431e-b64d-7a59b8962fae"
      },
      "source": [
        "# FLAN-T5 Fine_Tuning with PEFT & Building Retrival Augmented Generation (RAG) using Pincone vector database and vector embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a229d9f9",
      "metadata": {
        "id": "a229d9f9"
      },
      "source": [
        "### Import  the Needed Packages for Building the RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74d8975-53e1-41a3-a846-1fed3361a4b8",
      "metadata": {
        "height": 48,
        "id": "c74d8975-53e1-41a3-a846-1fed3361a4b8"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1382dbd-5b93-419e-ae16-735aa4e0f22a",
      "metadata": {
        "height": 167,
        "id": "e1382dbd-5b93-419e-ae16-735aa4e0f22a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tqdm.auto import tqdm\n",
        "from DLAIUtils import Utils\n",
        "\n",
        "import ast\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955e9783-9f33-4c09-8b10-ebee6b4a236e",
      "metadata": {
        "height": 65,
        "id": "955e9783-9f33-4c09-8b10-ebee6b4a236e"
      },
      "outputs": [],
      "source": [
        "# get api key\n",
        "utils = Utils()\n",
        "PINECONE_API_KEY = utils.get_pinecone_api_key()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c79cdc21",
      "metadata": {
        "id": "c79cdc21"
      },
      "source": [
        "### Setup Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e4c29d-79ee-4d6c-8a78-c7d29077fe5b",
      "metadata": {
        "height": 217,
        "id": "73e4c29d-79ee-4d6c-8a78-c7d29077fe5b"
      },
      "outputs": [],
      "source": [
        "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "utils = Utils()\n",
        "INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n",
        "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
        "  pinecone.delete_index(INDEX_NAME)\n",
        "\n",
        "pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',\n",
        "  spec=ServerlessSpec(cloud='aws', region='us-west-2'))\n",
        "\n",
        "index = pinecone.Index(INDEX_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94f00979",
      "metadata": {
        "id": "94f00979"
      },
      "source": [
        "### Load the Dataset\n",
        "\n",
        "**Note:** To download the dataset, just copy the following two lines of code and run it (remember to uncomment them first before executing):\n",
        "\n",
        "#!wget -q -O lesson2-wiki.csv.zip \"https://www.dropbox.com/scl/fi/yxzmsrv2sgl249zcspeqb/lesson2-wiki.csv.zip?rlkey=paehnoxjl3s5x53d1bedt4pmc&dl=0\"\n",
        "\n",
        "#!unzip lesson2-wiki.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b059ccc3-f6ba-49dd-a42c-bd9a5bcfc4d3",
      "metadata": {
        "id": "b059ccc3-f6ba-49dd-a42c-bd9a5bcfc4d3"
      },
      "source": [
        "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>(Note: <code>max_articles_num = 500</code>):</b> To achieve a more comprehensive context for the Language Learning Model, a larger number of articles is generally more beneficial. In this lab, we've initially set <code>max_articles_num</code> to 500 for speedier results, allowing you to observe the outcomes faster. Once you've done an initial run, consider increasing this value to 750 or 1,000. You'll likely notice that the context provided to the LLM becomes richer and better. You can experiment by gradually raising this variable for different queries to observe the improvements in the LLM's contextual understanding.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a372beb7-2322-4f6c-bb80-a9aa2c74fdaf",
      "metadata": {
        "height": 82,
        "id": "a372beb7-2322-4f6c-bb80-a9aa2c74fdaf",
        "outputId": "28910df8-137f-4fc8-a8f4-45102d9d1fba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>metadata</th>\n",
              "      <th>values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-0</td>\n",
              "      <td>{'chunk': 0, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>[-0.011254455894231796, -0.01698738895356655, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-1</td>\n",
              "      <td>{'chunk': 1, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>[-0.0015197008615359664, -0.007858820259571075...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1-2</td>\n",
              "      <td>{'chunk': 2, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>[-0.009930099360644817, -0.012211072258651257,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1-3</td>\n",
              "      <td>{'chunk': 3, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>[-0.011600767262279987, -0.012608098797500134,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1-4</td>\n",
              "      <td>{'chunk': 4, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>[-0.026462381705641747, -0.016362832859158516,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                           metadata  \\\n",
              "1  1-0  {'chunk': 0, 'source': 'https://simple.wikiped...   \n",
              "2  1-1  {'chunk': 1, 'source': 'https://simple.wikiped...   \n",
              "3  1-2  {'chunk': 2, 'source': 'https://simple.wikiped...   \n",
              "4  1-3  {'chunk': 3, 'source': 'https://simple.wikiped...   \n",
              "5  1-4  {'chunk': 4, 'source': 'https://simple.wikiped...   \n",
              "\n",
              "                                              values  \n",
              "1  [-0.011254455894231796, -0.01698738895356655, ...  \n",
              "2  [-0.0015197008615359664, -0.007858820259571075...  \n",
              "3  [-0.009930099360644817, -0.012211072258651257,...  \n",
              "4  [-0.011600767262279987, -0.012608098797500134,...  \n",
              "5  [-0.026462381705641747, -0.016362832859158516,...  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_articles_num = 500\n",
        "df = pd.read_csv('./data/wiki.csv', nrows=max_articles_num)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617fe45f",
      "metadata": {
        "id": "617fe45f"
      },
      "source": [
        "### Connect to OpenAI and embedding module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f1922a-122e-4f3d-a9b4-6ab1e5a67195",
      "metadata": {
        "height": 115,
        "id": "20f1922a-122e-4f3d-a9b4-6ab1e5a67195"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = utils.get_openai_api_key()\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def get_embeddings(articles, model=\"text-embedding-ada-002\"):\n",
        "   return openai_client.embeddings.create(input = articles, model=model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Needed Pakages for making PEFT LLM model out of Google FLAN-T5 model"
      ],
      "metadata": {
        "id": "J9JzBqQlQiwD"
      },
      "id": "J9JzBqQlQiwD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4677e60a",
      "metadata": {
        "height": 252,
        "id": "4677e60a",
        "outputId": "9f2d1718-ab67-4540-d4d0-5261e5536134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting datasets==2.17.0\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets==2.17.0)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets==2.17.0) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
            "Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
            "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pinecone-datasets 0.5.0rc11 requires pyarrow<12.0.0,>=11.0.0, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.17.0 pyarrow-16.1.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.3.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.10 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed pip-24.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.16.1 requires torch==2.1.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U datasets==2.17.0\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "eiWXf8XyRQNa"
      },
      "id": "eiWXf8XyRQNa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55848a87",
      "metadata": {
        "height": 149,
        "id": "55848a87",
        "outputId": "78d6cc1c-cb80-4e4c-e17b-f976817c2692"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-26 11:57:32.753594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-26 11:57:32.753638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-26 11:57:32.755026: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-26 11:57:32.762457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-26 11:57:33.833199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Dialog Summariztion dataset from Hugging Face"
      ],
      "metadata": {
        "id": "POLG28QzRamB"
      },
      "id": "POLG28QzRamB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This dataset contains a training,testing, and validating datasets which fits to the goal of training our FLAN-T5 model"
      ],
      "metadata": {
        "id": "-kJ_XBGBR5dB"
      },
      "id": "-kJ_XBGBR5dB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6517638a",
      "metadata": {
        "height": 116,
        "colab": {
          "referenced_widgets": [
            "f6fb36d0755d406d8116c15d4656cebc",
            "9b2cf52e2111472faeb9e665ad976291",
            "0238035d7ea7413c8be201b4fb5ddc35",
            "c77227cb8cb24b61b59652fa53cfaabf",
            "5b43726799b54feda756e4f17cd43cee",
            "00500f64ccd54dd3a70215d0f7328dea",
            "22631be8142245bd94832cc27e176028"
          ]
        },
        "id": "6517638a",
        "outputId": "c738809c-ae7e-4c37-8609-7eaede87c3ad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6fb36d0755d406d8116c15d4656cebc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b2cf52e2111472faeb9e665ad976291",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0238035d7ea7413c8be201b4fb5ddc35",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c77227cb8cb24b61b59652fa53cfaabf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b43726799b54feda756e4f17cd43cee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00500f64ccd54dd3a70215d0f7328dea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22631be8142245bd94832cc27e176028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the LLM base model"
      ],
      "metadata": {
        "id": "T-9GiZ5FSLos"
      },
      "id": "T-9GiZ5FSLos"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de0f7a6",
      "metadata": {
        "height": 115,
        "colab": {
          "referenced_widgets": [
            "d0990aed1dfd496b987e7814b80d7809",
            "b4a40abf4ed24669a3767a8f265cc651",
            "589db9b56218438db593afee9fc62e3e",
            "0974c0d923c7466c88991e8ce0e9a16d",
            "4a832dcf5c854e869f270e624a0d74e0",
            "7fa0a05dc1a64dcb8ecb4b3381dcc651",
            "c4949acd1c314777888c81e39ac2ac06"
          ]
        },
        "id": "3de0f7a6",
        "outputId": "89b002a1-9c8f-4816-e535-a1f69c47758e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0990aed1dfd496b987e7814b80d7809",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4a40abf4ed24669a3767a8f265cc651",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "589db9b56218438db593afee9fc62e3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0974c0d923c7466c88991e8ce0e9a16d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a832dcf5c854e869f270e624a0d74e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fa0a05dc1a64dcb8ecb4b3381dcc651",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4949acd1c314777888c81e39ac2ac06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9c2878",
      "metadata": {
        "height": 200,
        "id": "af9c2878",
        "outputId": "663d35d0-d959-46a8-9ff2-7f496f99c714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 3538944\n",
            "all model parameters: 251116800\n",
            "percentage of trainable model parameters: 1.41%\n"
          ]
        }
      ],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the base model on a random sample and comparing it with the human summarization"
      ],
      "metadata": {
        "id": "fULiu-yyVV2n"
      },
      "id": "fULiu-yyVV2n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3faf5df8",
      "metadata": {
        "height": 507,
        "id": "3faf5df8",
        "outputId": "d51672e8-6640-4f79-ce1f-dc3889728f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "#Person1#: There's a car waiting for you just outside the door. Right this way, please.\n",
            "#Person2#: OK!\n",
            "#Person1#: Let me put your cases into the trunk and please get in the back.\n",
            "#Person2#: Thanks!\n",
            "#Person1#: How was your flight?\n",
            "#Person2#: It's comfortable, but now I'm a little tired.\n",
            "#Person1#: We'll reach the Beijing hotel in another ten minutes. When we arrived there, you can go up and have a rest. The hotel has very good service, and it's considered as one of the best hotels here.\n",
            "#Person2#: Thank you! I lived there when I came to Beijing last time. It's comfortable and beautiful.\n",
            "#Person1#: If it's convenient for you, Mr. Wu would like to invite you to the banquet in honor of you in the evening.\n",
            "#Person2#: Thank you! I will. When and where will the dinner be?\n",
            "#Person1#: At six o'clock in the International Hotel. We'll pick you up this afternoon. Besides, if you care for visiting, we'll arrange some sightseeing for you.\n",
            "#Person2#: Oh, that's nice. Thank you for arranging all of this.\n",
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# warmly welcomes #Person2# and drives #Person2# to the Beijing Hotel. Mr. Wu has arranged a banquet for #Person2# in the evening.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "The car will pick you up at the Beijing hotel in 10 minutes.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "# Define function to generate summary\n",
        "def generate_summary(dialogue, model, tokenizer):\n",
        "    prompt = f\"\"\"\n",
        "    Summarize the following conversation.\n",
        "\n",
        "    {dialogue}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    generation_config = GenerationConfig(max_new_tokens=200)\n",
        "    output_ids = model.generate(inputs[\"input_ids\"], generation_config=generation_config)\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example dialogue and summary from dataset\n",
        "index = 250\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "# Generate summary using the model\n",
        "model_output = generate_summary(dialogue, original_model, tokenizer)\n",
        "\n",
        "# Print results\n",
        "dash_line = '-' * 80\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{model_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the model makes a good performance by adding an important info as the duration of the pick up while it produces a little short summary for the conversation"
      ],
      "metadata": {
        "id": "EIgGk-rIVrgV"
      },
      "id": "EIgGk-rIVrgV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c435c0f",
      "metadata": {
        "height": 166,
        "id": "6c435c0f"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation within 2 lines maximum.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "954a0519",
      "metadata": {
        "height": 98,
        "colab": {
          "referenced_widgets": [
            "23ea1d2e93da4390b38229a99ee84e41",
            "91b8fd607bcf48079020fcdffdb3c89c",
            "aaf94d3d53ed4a5c805370da0bc9dd74"
          ]
        },
        "id": "954a0519",
        "outputId": "457c87a7-00be-4194-d84f-0bdd4591fa62"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23ea1d2e93da4390b38229a99ee84e41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91b8fd607bcf48079020fcdffdb3c89c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaf94d3d53ed4a5c805370da0bc9dd74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb784e1",
      "metadata": {
        "height": 234,
        "colab": {
          "referenced_widgets": [
            "a37e1269887e43b09b281eec1f94666a",
            "89589bde73a74f6ab112bf0d65cf4b38",
            "26a3acb0b83245b5bcd5cf25a0bce02b"
          ]
        },
        "id": "9bb784e1",
        "outputId": "f72f9eaf-9a49-43b4-83c0-11d09efae983"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a37e1269887e43b09b281eec1f94666a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89589bde73a74f6ab112bf0d65cf4b38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26a3acb0b83245b5bcd5cf25a0bce02b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of the datasets:\n",
            "Training: (125, 2)\n",
            "Validation: (5, 2)\n",
            "Test: (15, 2)\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 125\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 5\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'labels'],\n",
            "        num_rows: 15\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
        "\n",
        "\n",
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)\n",
        "\n",
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the base model on the dataset we previously imported"
      ],
      "metadata": {
        "id": "kOGInE-XWMxR"
      },
      "id": "kOGInE-XWMxR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d59d6ef0",
      "metadata": {
        "height": 286,
        "id": "d59d6ef0"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2667da80",
      "metadata": {
        "height": 31,
        "id": "2667da80",
        "outputId": "977b7df2-50f8-4b47-a707-7893dd55ce43"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>50.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1, training_loss=50.5, metrics={'train_runtime': 178.6814, 'train_samples_per_second': 0.045, 'train_steps_per_second': 0.006, 'total_flos': 5478058819584.0, 'train_loss': 50.5, 'epoch': 0.06})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing our specified trained model on a sample conversation  "
      ],
      "metadata": {
        "id": "Gmqmn1UBWhyk"
      },
      "id": "Gmqmn1UBWhyk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25230a18",
      "metadata": {
        "height": 438,
        "id": "25230a18",
        "outputId": "c2e2a34a-b285-4665-94dc-4d4fdd261ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# is crazy for Trump and voted for him. #Person2# doesn't agree with #Person1# on Trump and will vote for Biden.\n",
            "--------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL:\n",
            "#Person1: I cannot imagine if Trump were to be our President again.\n"
          ]
        }
      ],
      "source": [
        "index = 300\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Adjust the generation configuration for better summarization\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=150,  # Increase the max_new_tokens to allow for a longer summary\n",
        "    num_beams=5,  # Use beam search for better results\n",
        "    no_repeat_ngram_size=2,  # Prevent repetition\n",
        "    length_penalty=2.0,  # Encourage longer summaries\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "dash_line = '-' * 80\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Fine-Tuned model to the trained LLM to increase it's relevance content using LORA and PEFT"
      ],
      "metadata": {
        "id": "eE9XRBHZWt4E"
      },
      "id": "eE9XRBHZWt4E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9675f2af",
      "metadata": {
        "height": 184,
        "id": "9675f2af"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34224e48",
      "metadata": {
        "height": 65,
        "id": "34224e48",
        "outputId": "e5da496c-730f-43f8-e53c-6ef8d28aa0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable model parameters: 3538944\n",
            "all model parameters: 251116800\n",
            "percentage of trainable model parameters: 1.41%\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(original_model,\n",
        "                            lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Adding the PEFT model to a directory"
      ],
      "metadata": {
        "id": "ucAzoZVzXKfS"
      },
      "id": "ucAzoZVzXKfS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1a8689",
      "metadata": {
        "height": 302,
        "id": "1c1a8689"
      },
      "outputs": [],
      "source": [
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c99dc93",
      "metadata": {
        "height": 150,
        "id": "4c99dc93",
        "outputId": "27cdbe86-0a88-414b-f9bc-20ab433e8530"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>45.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
              " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
              " './peft-dialogue-summary-checkpoint-local/spiece.model',\n",
              " './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n",
              " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the PEFT model results"
      ],
      "metadata": {
        "id": "eZMqconhXVu-"
      },
      "id": "eZMqconhXVu-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8da5fc",
      "metadata": {
        "height": 540,
        "id": "ba8da5fc",
        "outputId": "72c15047-a9db-4d47-e4ab-fe0560338a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
            "--------------------------------------------------------------------------------\n",
            "ORIGINAL MODEL:\n",
            "#Person1: You could consider adding a painting program to your software. You might also want to upgrade your hardware because it is outdated now.\n",
            "--------------------------------------------------------------------------------\n",
            "PEFT MODEL: #Person1#: Have you considered upgrading your hardware?\n"
          ]
        }
      ],
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation briefly.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Adjust the generation configuration for better summarization\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=50,  # Limit the number of new tokens to keep the summary concise\n",
        "    num_beams=3,  # Use a smaller number of beams to find concise summaries\n",
        "    no_repeat_ngram_size=2,  # Prevent repetition\n",
        "    length_penalty=1.5,  # Encourage concise summaries\n",
        "    early_stopping=True,\n",
        "    temperature=0.7,  # Slightly increase temperature for diversity\n",
        "    top_p=0.9  # Use top-p sampling to limit the pool of tokens\n",
        ")\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "dash_line = '-' * 80\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: {peft_model_text_output}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa307525",
      "metadata": {
        "id": "fa307525"
      },
      "source": [
        "## Prepare the Embeddings and Upsert to Pinecone on the Wikipedia dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note we are using the wikipedia dataset for more generalized topics and to test the PEFT model on a new topic to see how will it responds and summarizes these wikipidia articles"
      ],
      "metadata": {
        "id": "qSxo8iEaYA8j"
      },
      "id": "qSxo8iEaYA8j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ac4cb4-e900-4d9e-bfc1-cd0ada96ed80",
      "metadata": {
        "height": 235,
        "colab": {
          "referenced_widgets": [
            "aa1b92de29eb46029385b479fcd61713"
          ]
        },
        "id": "27ac4cb4-e900-4d9e-bfc1-cd0ada96ed80",
        "outputId": "e0efb3fb-8952-42f0-82d7-0c992b601fcf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa1b92de29eb46029385b479fcd61713",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepped = []\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    meta = ast.literal_eval(row['metadata'])\n",
        "    prepped.append({'id':row['id'],\n",
        "                    'values':ast.literal_eval(row['values']),\n",
        "                    'metadata':meta})\n",
        "    if len(prepped) >= 250:\n",
        "        index.upsert(prepped)\n",
        "        prepped = []\n",
        "\n",
        "\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7180b6b2-ad7c-4a59-ab6c-046d21957343",
      "metadata": {
        "height": 31,
        "id": "7180b6b2-ad7c-4a59-ab6c-046d21957343",
        "outputId": "f49a0472-e7f0-4929-d175-6a6fb8204241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 500}},\n",
              " 'total_vector_count': 500}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f629efde",
      "metadata": {
        "id": "f629efde"
      },
      "source": [
        "### Run Your Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f790629-f7e4-40e8-9f96-cfdd46cfde9c",
      "metadata": {
        "height": 285,
        "id": "3f790629-f7e4-40e8-9f96-cfdd46cfde9c",
        "outputId": "d330dcf0-e094-4138-9ca4-4daa45fe7b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Egon Krenz was elected by the politburo to be Honecker's successor. Krenz tried to show that he was looking for change within the GDR but the citizens did not trust him. On November 9, 1989, the SED announced that East Germans would be able to travel to West Berlin the next day. The spokesman who announced the new travel law incorrectly said that it would take effect immediately, implying the Berlin Wall would open that night. People began to gather at border checkpoints at the wall hoping to be let through, but the guards told them that they had no orders to let citizens through. As the number of people grew, the guards became alarmed and tried to contact their superiors but had no responses. Unwilling to use force, the chief guard at the checkpoint relented at 10:54pm and ordered the gate to be opened. Thousands of East-Germans swarmed into West Berlin and the purpose of the wall was deemed now obsolete. The fall of the wall destroyed the SED politically as well as the career of its leader, Egon Krenz. On December 1, 1989, the GDR government revoked the law that guaranteed the SED the right to rule the East German political system, effectively ending communist rule in the GDR.\n",
            "\n",
            "On 18 March 1990, there were free elections in the GDR. The \"Alliance for Germany\", a group of political parties who wanted to unify the GDR with West Germany, won that election. This process, when East Germany was taken over by the West, is known also the Wende in Germany.\n",
            "Berlin (; ) is the capital city of Germany. It is the largest city in the European Union by population, with around 3.7 million people in 2020. Berlin residents come from about 190 different countries.\n",
            "\n",
            "The city is in the eastern part of Germany in Central Europe and is surrounded by many forests and lakes. Berlin has an area of . The rivers Havel, Dahme and Spree run through Berlin. It has a temperate climate.\n",
            "\n",
            "Berlin is home to many famous buildings and monuments, like the Siegessäule, the Brandenburger Tor, the Reichstag and the boulevard Unter den Linden. On the boulevard is the Humboldt University. The city has many nightlife options.\n",
            "\n",
            "Berlin is an important city for the history of Germany. The King of Prussia and the Emperor of Germany lived in Berlin. The government of Germany was in Berlin for many years. Bombs destroyed many buildings in the city in World War Two. The city was split into West Berlin and East Berlin after World War Two. After the Berlin Wall was built in 1961 very few people were allowed to cross from East Berlin into West Berlin. The wall divided the city until 1989 when the East German government decided to allow anyone to cross, and people decided to tear down the wall.\n",
            "\n",
            "Berlin is a world city of culture, start ups, politics, media and science. There are a lot of technology companies in the city. They are important for the city's economy. Many planes and trains travel to and from Berlin because the city is an important place for tourism and business.\n",
            "The German Democratic Republic (GDR) ( (DDR)), commonly called East Germany (), was founded on 7 October 1949, after World War II. It was formed from part of the Soviet occupation zone of Germany, including part of the city of Berlin. It is no longer a nation by itself since the two parts of Germany, East Germany and West Germany, reunified in 1990.\n",
            "\n",
            "The GDR was ruled by the Socialist Unity Party of Germany (SED).\n",
            "\n",
            "History \n",
            "After World War II, the four Allied Occupation Zones in Germany were each controlled by a different country. The countries that controlled these parts of Germany were France, the United Kingdom, the United States, and the Soviet Union. The French, American, and British parts of Germany formed West Germany (the Bundesrepublik). Part of the Soviet section became East Germany, and other parts became western Poland and small parts of other countries.\n",
            "\n",
            "Walter Ulbricht, the head of the SED, also had a lot of power. Pieck died in 1960, and Ulbricht became \"Chairman of the State Council\". Now he was really the head of state.\n",
            "\n",
            "On 13 August 1961, the Berlin Wall was built.  Many people were shot dead by East German soldiers when they tried to escape the GDR.  According to the SED this was to make it hard for American spies to use West Berlin as a place to work from, but it also made it hard for normal people to move between east and west.\n",
            "History \n",
            " 1244 The first writings about a place called Berlin.\n",
            " 1451 The Hohenzollern family moved to Berlin as the place to rule their country\n",
            " 1618 – 48 After the Thirty Years' War in Germany, the number of people that lived in Berlin dropped to only 6000.\n",
            " 1701 Berlin became capital of Prussia.\n",
            " 1709 Berlin and its neighbor city Cölln (not Köln/Cologne) are combined to form the new Berlin.\n",
            " 1806 The army of Napoleon Bonaparte moved into Berlin.\n",
            " 1871 Berlin became capital of the German Empire.\n",
            " 1920 The Old Berlin and some neighbour towns are combined into \"Greater Berlin\" (Groß-Berlin).\n",
            " 1945 The town is divided into four sectors, used by the allies of World War II. There is a Soviet Sector, an American Sector, a British Sector and a French Sector.\n",
            " 1949 After foundation of the two Germanies, the town is still divided. There is now West Berlin and East Berlin.\n",
            " 1961 The Berlin Wall was built by the communist government of East Germany between the two halves of Berlin.\n",
            " 1990 After German reunification, the Berlin Wall is torn down, and there is only one Berlin again. The new Berlin becomes the capital of one Germany.\n",
            " 2001 23 boroughs of Berlin were changed into 12\n",
            " 2006 FIFA World Cup Final held at Olympiastadion\n",
            "\n",
            "People \n",
            "\n",
            "In 2018, Berlin had 3.75 million registered inhabitants in an area of . The city's population density was 4,206 inhabitants per km2. Berlin is the most populous city in Germany an the European Union. The entire Berlin-Brandenburg capital region has a population of more than 6 million in an area of . More than 2.0 million households were counted in the city. Around 60 percent of them were single-person households.\n",
            "Landmarks \n",
            " Alexanderplatz \n",
            " Berliner Dom (Berlin's cathedral)\n",
            " Berlin Hauptbahnhof (Main Railway station)\n",
            " Brandenburg Gate\n",
            " East Side Gallery (Former Berlin Wall)\n",
            " Fernsehturm (TV tower - the highest building in Germany)\n",
            " Potsdamer Platz\n",
            " Reichstag building (home of the Bundestag)\n",
            " Rotes Rathaus (office of the Governing Mayor)\n",
            " Siegessäule (Statue of Victory)\n",
            " Tiergarten (Central Park)\n",
            " Unter den Linden (Main boulevard)\n",
            "\n",
            "Cuisine \n",
            "\n",
            "The Berlin cuisine and culinary offerings vary greatly. 23 restaurants in Berlin have been awarded one or more Michelin stars in 2021, which ranks the city at the top for the number of restaurants in Germany. \n",
            "\n",
            "Many local foods originated from north German culinary traditions and include rustic and hearty dishes with pork, goose, fish, peas, beans, cucumbers, or potatoes. Typical Berliner fare include popular street food like the Currywurst Buletten (meat balls) and the Berliner doughnut, known in Berlin as . German bakeries offering a variety of breads and pastries are widespread. One of Europe's largest delicatessen market is found at the department store KaDeWe. Among the world's largest chocolate stores is Fassbender & Rausch.\n",
            "\n",
            "Berlin is also home to a diverse gastronomy scene reflecting the immigrant history of the city. Immigrants brought their culinary traditions to the city, such as the modern fast-food version of the doner kebab. Asian cuisine like Chinese, Vietnamese, Thai, Indian, Korean, and Japanese restaurants, as well as Spanish tapas bars, Italian, and Greek cuisine, can be found in many parts of the city.\n",
            "\n",
            "Economy\n",
            "There are more than 20 communities with a population of at least 10,000 people in 2019, including German, Turkish, Polish, Syrian, Italian, Bulgarian, Russian, Lebanese, Palestinian, Serbian, Bosnian, Vietnamese, American, Romanian, Croatian, Chinese, Austrian, Ukrainian, French, British, Spanish, Israeli, Indian and Iranian communities.\n",
            "\n",
            "In 2019, there were 777,345 registered residents of foreign nationality and another 542,975 German citizens with a \"migration background\", meaning they or one of their parents immigrated to Germany after 1955. Berlin residents originate from about 190 different countries.\n",
            "\n",
            "Geography\n",
            "\n",
            "Berlin is in northeastern Germany, in an area of low-lying marshy woodlands with a mainly flat terrain. It is part of the Northern European Plain. The river Spree and Havel are the two main rivers in the city. In Spandau, a borough in the west of Berlin, the Spree empties into the river Havel, which flows from north to south through western Berlin. The largest lakes being the Tegeler See, the Großer Wannsee and the Großer Müggelsee.\n",
            "\n",
            "The Arkenberge hills in Pankow at  elevation are the highest point in Berlin. The Müggelberge (mountains) at  elevation is the highest natural point and the lowest is the Spektesee in Spandau, at  elevation.\n",
            "\n",
            "Boroughs \n",
            "\n",
            " Charlottenburg-Wilmersdorf\n",
            " Friedrichshain-Kreuzberg\n",
            " Lichtenberg-Hohenschönhausen\n",
            " Marzahn-Hellersdorf\n",
            " Mitte (Central)\n",
            " Neukölln\n",
            " Pankow\n",
            " Reinickendorf\n",
            " Spandau\n",
            " Steglitz-Zehlendorf\n",
            " Tempelhof-Schöneberg\n",
            " Treptow-Köpenick\n",
            "\n",
            "Education\n",
            "In the German reunification, the GDR joined West Germany by approving its constitution in 1990. The East German districts were reorganised into the Länder (Berlin, Brandenburg, Mecklenburg-Vorpommern, Sachsen, Sachsen-Anhalt and Thüringen) and joined West Germany, after which the GDR ceased to exist. Fidel Castro had long ago renamed the small Cuban island of Cayo Blanco del Sur and one of its beaches in honor of the GDR, though it remained part of Cuba.\n",
            "\n",
            "Even though the western and the eastern part joined back together in 1990, people from former West Germany still call people from East Germany \"Ossi\". This comes from the German word \"Osten\" which means \"East\". Ossi is not always meant kindly.\n",
            "\n",
            "After the reunification, many people became angry because the new government was from the west and didn't like East Germany. They closed down lots of the places people worked and tried to make it look like East Germany never existed. This made lots of people lose their jobs and become poor. Today lots of people who used to live in East Germany want it to come back. This is called \"Ostalgie\", which means \"East nostalgia\".\n",
            "Economy\n",
            "\n",
            "In 2018, the GDP of Berlin totaled €147 billion. The city is the largest metropolitan economy in Germany and the third largest in the European Union. Berlin's economy is dominated by the service sector, with around 85% of all companies doing business in services. In 2019, the total labor force in Berlin was about 2.0 million. \n",
            "\n",
            "Important economic sectors in Berlin include life sciences, transportation, information and communication technologies, media and music, advertising and design, biotechnology, environmental services, construction, e-commerce, retail, hotel business, and medical engineering.\n",
            "\n",
            "Research and development are important for the city. Berlin is part of the Eurozone.\n",
            "\n",
            "Sister cities \n",
            "Berlin has partnerships with 17 cities. Each of the 12 boroughs also has their sister cities, sometimes called twin cities.\n",
            "\n",
            "References\n",
            "\n",
            "Other websites \n",
            "\n",
            " - Official page www.berlin.de\n",
            " Berlin Sightseeing Tours\n",
            " EXBERLINER - English City Magazine\n",
            " Berlin City Panoramas - Panoramic Views and virtual Tours of Berlin\n",
            "\n",
            " \n",
            "Olympic cities\n",
            "Education\n",
            "\n",
            "Berlin is one of the most renowned centers of higher education and research in Germany and the world. Historically, 57 Nobel Prize winners are affiliated with the Berlin-based universities.\n",
            "\n",
            "The city has four universities and more than 40 private, professional, and technical colleges in 2020. Around 200.000 students were enrolled in 2019. Among them around 20% have an international background.\n",
            "\n",
            "The three largest universities combined have approximately 110,000 students. There are the Free University of Berlin (Free University of Berlin, FU Berlin) with about 35,000 students, the Humboldt University of Berlin (HU Berlin) with 40,000 students, and the Technical University of Berlin (TU Berlin) with 35,000 students. The Charité Medical School has around 9,000 students. The Berlin University of the Arts (UdK) has about 4,000 students and the ESMT Berlin is a leading business schools in Germany. The Berlin School of Economics and Law (HWR) has an enrollment of about 11,000 students, the Berlin University of Applied Sciences and Technology (BHT) of about 12,000 students, and the Hochschule für Technik und Wirtschaft (University of Applied Sciences for Engineering and Economics, HTW) of about 14,000 students.\n",
            "\n",
            "Culture \n",
            "\n",
            "Berlin is famous for its numerous cultural institutions, many of which enjoy international reputation. It is a trendsetting city. Young people, creatives and entrepreneurs continue to settle in the city and make Berlin a popular entertainment center in the world.\n",
            "August 13  1961: Building of the Berlin Wall begins.\n",
            " August 14  1945: Japan announces its surrender at the end of World War II.\n",
            " August 14/15  1947: India is partitioned at independence from the UK, as the new mainly Islamic state of Pakistan is created.\n",
            " August 15  1960: The Republic of the Congo becomes independent.\n",
            " August 15  1971: Bahrain becomes independent.\n",
            " August 16  1977: Elvis Presley dies aged 42, leading to a worldwide outpouring of grief.\n",
            " August 17  1945: Indonesia declares independence from the Netherlands.\n",
            " August 17  1960: Gabon becomes independent.\n",
            " August 17  1962: Peter Fechter becomes the first person to be shot dead at the Berlin Wall.\n",
            " August 19  43 BC: Augustus becomes Roman consul.\n",
            " August 19  14: Augustus dies.\n",
            " August 19  1919: Afghanistan becomes independent.\n",
            " August 19  1991: The August Coup against Mikhail Gorbachev, in the Soviet Union, begins.\n",
            " August 20  1940: Leon Trotsky is fatally wounded with an ice pick in Mexico.\n",
            " August 20  1968: The Prague Spring uprising is crushed.\n",
            " August 20  1991: Estonia regains its independence from the Soviet Union.\n",
            " August 21  1959: Hawaii becomes the 50th State of the US.\n",
            " August 24  79: Vesuvius erupts, destroying Pompeii and neighbouring Herculaneum.\n",
            " August 24  1991: Ukraine regains independence from the Soviet Union.\n",
            " August 25  1825: Uruguay declares independence from Brazil.\n"
          ]
        }
      ],
      "source": [
        "query = \"what is the berlin wall?\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following topic.\n",
        "\n",
        "{query}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "embed = get_embeddings([query])\n",
        "res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n",
        "text = [r['metadata']['text'] for r in res['matches']]\n",
        "print('\\n'.join(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ee483a",
      "metadata": {
        "id": "26ee483a"
      },
      "source": [
        "### Build the Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92cdddce-9a39-4ffd-823c-e91b7af48cf5",
      "metadata": {
        "height": 438,
        "id": "92cdddce-9a39-4ffd-823c-e91b7af48cf5",
        "outputId": "57d6fb21-692c-49c8-f095-c7dab3e88677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the context below.\n",
            "\n",
            "Context:\n",
            "Egon Krenz was elected by the politburo to be Honecker's successor. Krenz tried to show that he was looking for change within the GDR but the citizens did not trust him. On November 9, 1989, the SED announced that East Germans would be able to travel to West Berlin the next day. The spokesman who announced the new travel law incorrectly said that it would take effect immediately, implying the Berlin Wall would open that night. People began to gather at border checkpoints at the wall hoping to be let through, but the guards told them that they had no orders to let citizens through. As the number of people grew, the guards became alarmed and tried to contact their superiors but had no responses. Unwilling to use force, the chief guard at the checkpoint relented at 10:54pm and ordered the gate to be opened. Thousands of East-Germans swarmed into West Berlin and the purpose of the wall was deemed now obsolete. The fall of the wall destroyed the SED politically as well as the career of its leader, Egon Krenz. On December 1, 1989, the GDR government revoked the law that guaranteed the SED the right to rule the East German political system, effectively ending communist rule in the GDR.\n",
            "\n",
            "On 18 March 1990, there were free elections in the GDR. The \"Alliance for Germany\", a group of political parties who wanted to unify the GDR with West Germany, won that election. This process, when East Germany was taken over by the West, is known also the Wende in Germany.\n",
            "\n",
            "---\n",
            "\n",
            "Berlin (; ) is the capital city of Germany. It is the largest city in the European Union by population, with around 3.7 million people in 2020. Berlin residents come from about 190 different countries.\n",
            "\n",
            "The city is in the eastern part of Germany in Central Europe and is surrounded by many forests and lakes. Berlin has an area of . The rivers Havel, Dahme and Spree run through Berlin. It has a temperate climate.\n",
            "\n",
            "Berlin is home to many famous buildings and monuments, like the Siegessäule, the Brandenburger Tor, the Reichstag and the boulevard Unter den Linden. On the boulevard is the Humboldt University. The city has many nightlife options.\n",
            "\n",
            "Berlin is an important city for the history of Germany. The King of Prussia and the Emperor of Germany lived in Berlin. The government of Germany was in Berlin for many years. Bombs destroyed many buildings in the city in World War Two. The city was split into West Berlin and East Berlin after World War Two. After the Berlin Wall was built in 1961 very few people were allowed to cross from East Berlin into West Berlin. The wall divided the city until 1989 when the East German government decided to allow anyone to cross, and people decided to tear down the wall.\n",
            "\n",
            "Berlin is a world city of culture, start ups, politics, media and science. There are a lot of technology companies in the city. They are important for the city's economy. Many planes and trains travel to and from Berlin because the city is an important place for tourism and business.\n",
            "\n",
            "---\n",
            "\n",
            "History \n",
            " 1244 The first writings about a place called Berlin.\n",
            " 1451 The Hohenzollern family moved to Berlin as the place to rule their country\n",
            " 1618 – 48 After the Thirty Years' War in Germany, the number of people that lived in Berlin dropped to only 6000.\n",
            " 1701 Berlin became capital of Prussia.\n",
            " 1709 Berlin and its neighbor city Cölln (not Köln/Cologne) are combined to form the new Berlin.\n",
            " 1806 The army of Napoleon Bonaparte moved into Berlin.\n",
            " 1871 Berlin became capital of the German Empire.\n",
            " 1920 The Old Berlin and some neighbour towns are combined into \"Greater Berlin\" (Groß-Berlin).\n",
            " 1945 The town is divided into four sectors, used by the allies of World War II. There is a Soviet Sector, an American Sector, a British Sector and a French Sector.\n",
            " 1949 After foundation of the two Germanies, the town is still divided. There is now West Berlin and East Berlin.\n",
            " 1961 The Berlin Wall was built by the communist government of East Germany between the two halves of Berlin.\n",
            " 1990 After German reunification, the Berlin Wall is torn down, and there is only one Berlin again. The new Berlin becomes the capital of one Germany.\n",
            " 2001 23 boroughs of Berlin were changed into 12\n",
            " 2006 FIFA World Cup Final held at Olympiastadion\n",
            "\n",
            "People \n",
            "\n",
            "In 2018, Berlin had 3.75 million registered inhabitants in an area of . The city's population density was 4,206 inhabitants per km2. Berlin is the most populous city in Germany an the European Union. The entire Berlin-Brandenburg capital region has a population of more than 6 million in an area of . More than 2.0 million households were counted in the city. Around 60 percent of them were single-person households.\n",
            "\n",
            "---\n",
            "\n",
            "The German Democratic Republic (GDR) ( (DDR)), commonly called East Germany (), was founded on 7 October 1949, after World War II. It was formed from part of the Soviet occupation zone of Germany, including part of the city of Berlin. It is no longer a nation by itself since the two parts of Germany, East Germany and West Germany, reunified in 1990.\n",
            "\n",
            "The GDR was ruled by the Socialist Unity Party of Germany (SED).\n",
            "\n",
            "History \n",
            "After World War II, the four Allied Occupation Zones in Germany were each controlled by a different country. The countries that controlled these parts of Germany were France, the United Kingdom, the United States, and the Soviet Union. The French, American, and British parts of Germany formed West Germany (the Bundesrepublik). Part of the Soviet section became East Germany, and other parts became western Poland and small parts of other countries.\n",
            "\n",
            "Walter Ulbricht, the head of the SED, also had a lot of power. Pieck died in 1960, and Ulbricht became \"Chairman of the State Council\". Now he was really the head of state.\n",
            "\n",
            "On 13 August 1961, the Berlin Wall was built.  Many people were shot dead by East German soldiers when they tried to escape the GDR.  According to the SED this was to make it hard for American spies to use West Berlin as a place to work from, but it also made it hard for normal people to move between east and west.\n",
            "\n",
            "---\n",
            "\n",
            "Landmarks \n",
            " Alexanderplatz \n",
            " Berliner Dom (Berlin's cathedral)\n",
            " Berlin Hauptbahnhof (Main Railway station)\n",
            " Brandenburg Gate\n",
            " East Side Gallery (Former Berlin Wall)\n",
            " Fernsehturm (TV tower - the highest building in Germany)\n",
            " Potsdamer Platz\n",
            " Reichstag building (home of the Bundestag)\n",
            " Rotes Rathaus (office of the Governing Mayor)\n",
            " Siegessäule (Statue of Victory)\n",
            " Tiergarten (Central Park)\n",
            " Unter den Linden (Main boulevard)\n",
            "\n",
            "Cuisine \n",
            "\n",
            "The Berlin cuisine and culinary offerings vary greatly. 23 restaurants in Berlin have been awarded one or more Michelin stars in 2021, which ranks the city at the top for the number of restaurants in Germany. \n",
            "\n",
            "Many local foods originated from north German culinary traditions and include rustic and hearty dishes with pork, goose, fish, peas, beans, cucumbers, or potatoes. Typical Berliner fare include popular street food like the Currywurst Buletten (meat balls) and the Berliner doughnut, known in Berlin as . German bakeries offering a variety of breads and pastries are widespread. One of Europe's largest delicatessen market is found at the department store KaDeWe. Among the world's largest chocolate stores is Fassbender & Rausch.\n",
            "\n",
            "Berlin is also home to a diverse gastronomy scene reflecting the immigrant history of the city. Immigrants brought their culinary traditions to the city, such as the modern fast-food version of the doner kebab. Asian cuisine like Chinese, Vietnamese, Thai, Indian, Korean, and Japanese restaurants, as well as Spanish tapas bars, Italian, and Greek cuisine, can be found in many parts of the city.\n",
            "\n",
            "Economy\n",
            "\n",
            "---\n",
            "\n",
            "August 13  1961: Building of the Berlin Wall begins.\n",
            " August 14  1945: Japan announces its surrender at the end of World War II.\n",
            " August 14/15  1947: India is partitioned at independence from the UK, as the new mainly Islamic state of Pakistan is created.\n",
            " August 15  1960: The Republic of the Congo becomes independent.\n",
            " August 15  1971: Bahrain becomes independent.\n",
            " August 16  1977: Elvis Presley dies aged 42, leading to a worldwide outpouring of grief.\n",
            " August 17  1945: Indonesia declares independence from the Netherlands.\n",
            " August 17  1960: Gabon becomes independent.\n",
            " August 17  1962: Peter Fechter becomes the first person to be shot dead at the Berlin Wall.\n",
            " August 19  43 BC: Augustus becomes Roman consul.\n",
            " August 19  14: Augustus dies.\n",
            " August 19  1919: Afghanistan becomes independent.\n",
            " August 19  1991: The August Coup against Mikhail Gorbachev, in the Soviet Union, begins.\n",
            " August 20  1940: Leon Trotsky is fatally wounded with an ice pick in Mexico.\n",
            " August 20  1968: The Prague Spring uprising is crushed.\n",
            " August 20  1991: Estonia regains its independence from the Soviet Union.\n",
            " August 21  1959: Hawaii becomes the 50th State of the US.\n",
            " August 24  79: Vesuvius erupts, destroying Pompeii and neighbouring Herculaneum.\n",
            " August 24  1991: Ukraine regains independence from the Soviet Union.\n",
            " August 25  1825: Uruguay declares independence from Brazil.\n",
            "\n",
            "---\n",
            "\n",
            "After Mikhail Gorbachev had started glasnost and perestroika in the Soviet Union, many people in the GDR wanted reforms, too. In 1989, there were lots of demonstrations against the SED and for McDonalds and Nike. In the city of Leipzig, people met every Monday and demonstrated, and so these demonstrations are called Montagsdemonstrationen (\"Monday Demonstrations\"). Erich Honecker wished that the Soviets would use its army to suppress these demonstrations.  The Soviet Union, with its own political and economical problems, refused and did not want to help Eastern Europe anymore.  Honecker was eventually forced to resign on October 18, 1989.\n",
            "\n",
            "---\n",
            "\n",
            "Economy\n",
            "\n",
            "In 2018, the GDP of Berlin totaled €147 billion. The city is the largest metropolitan economy in Germany and the third largest in the European Union. Berlin's economy is dominated by the service sector, with around 85% of all companies doing business in services. In 2019, the total labor force in Berlin was about 2.0 million. \n",
            "\n",
            "Important economic sectors in Berlin include life sciences, transportation, information and communication technologies, media and music, advertising and design, biotechnology, environmental services, construction, e-commerce, retail, hotel business, and medical engineering.\n",
            "\n",
            "Research and development are important for the city. Berlin is part of the Eurozone.\n",
            "\n",
            "Sister cities \n",
            "Berlin has partnerships with 17 cities. Each of the 12 boroughs also has their sister cities, sometimes called twin cities.\n",
            "\n",
            "References\n",
            "\n",
            "Other websites \n",
            "\n",
            " - Official page www.berlin.de\n",
            " Berlin Sightseeing Tours\n",
            " EXBERLINER - English City Magazine\n",
            " Berlin City Panoramas - Panoramic Views and virtual Tours of Berlin\n",
            "\n",
            " \n",
            "Olympic cities\n",
            "\n",
            "---\n",
            "\n",
            "There are more than 20 communities with a population of at least 10,000 people in 2019, including German, Turkish, Polish, Syrian, Italian, Bulgarian, Russian, Lebanese, Palestinian, Serbian, Bosnian, Vietnamese, American, Romanian, Croatian, Chinese, Austrian, Ukrainian, French, British, Spanish, Israeli, Indian and Iranian communities.\n",
            "\n",
            "In 2019, there were 777,345 registered residents of foreign nationality and another 542,975 German citizens with a \"migration background\", meaning they or one of their parents immigrated to Germany after 1955. Berlin residents originate from about 190 different countries.\n",
            "\n",
            "Geography\n",
            "\n",
            "Berlin is in northeastern Germany, in an area of low-lying marshy woodlands with a mainly flat terrain. It is part of the Northern European Plain. The river Spree and Havel are the two main rivers in the city. In Spandau, a borough in the west of Berlin, the Spree empties into the river Havel, which flows from north to south through western Berlin. The largest lakes being the Tegeler See, the Großer Wannsee and the Großer Müggelsee.\n",
            "\n",
            "The Arkenberge hills in Pankow at  elevation are the highest point in Berlin. The Müggelberge (mountains) at  elevation is the highest natural point and the lowest is the Spektesee in Spandau, at  elevation.\n",
            "\n",
            "Boroughs \n",
            "\n",
            " Charlottenburg-Wilmersdorf\n",
            " Friedrichshain-Kreuzberg\n",
            " Lichtenberg-Hohenschönhausen\n",
            " Marzahn-Hellersdorf\n",
            " Mitte (Central)\n",
            " Neukölln\n",
            " Pankow\n",
            " Reinickendorf\n",
            " Spandau\n",
            " Steglitz-Zehlendorf\n",
            " Tempelhof-Schöneberg\n",
            " Treptow-Köpenick\n",
            "\n",
            "Education\n",
            "\n",
            "---\n",
            "\n",
            "Education\n",
            "\n",
            "Berlin is one of the most renowned centers of higher education and research in Germany and the world. Historically, 57 Nobel Prize winners are affiliated with the Berlin-based universities.\n",
            "\n",
            "The city has four universities and more than 40 private, professional, and technical colleges in 2020. Around 200.000 students were enrolled in 2019. Among them around 20% have an international background.\n",
            "\n",
            "The three largest universities combined have approximately 110,000 students. There are the Free University of Berlin (Free University of Berlin, FU Berlin) with about 35,000 students, the Humboldt University of Berlin (HU Berlin) with 40,000 students, and the Technical University of Berlin (TU Berlin) with 35,000 students. The Charité Medical School has around 9,000 students. The Berlin University of the Arts (UdK) has about 4,000 students and the ESMT Berlin is a leading business schools in Germany. The Berlin School of Economics and Law (HWR) has an enrollment of about 11,000 students, the Berlin University of Applied Sciences and Technology (BHT) of about 12,000 students, and the Hochschule für Technik und Wirtschaft (University of Applied Sciences for Engineering and Economics, HTW) of about 14,000 students.\n",
            "\n",
            "Culture \n",
            "\n",
            "Berlin is famous for its numerous cultural institutions, many of which enjoy international reputation. It is a trendsetting city. Young people, creatives and entrepreneurs continue to settle in the city and make Berlin a popular entertainment center in the world.\n",
            "\n",
            "Question: write an article titled: what is the berlin wall?\n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "query = \"write an article titled: what is the berlin wall?\"\n",
        "embed = get_embeddings([query])\n",
        "res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n",
        "\n",
        "contexts = [\n",
        "    x['metadata']['text'] for x in res['matches']\n",
        "]\n",
        "\n",
        "prompt_start = (\n",
        "    \"Answer the question based on the context below.\\n\\n\"+\n",
        "    \"Context:\\n\"\n",
        ")\n",
        "\n",
        "prompt_end = (\n",
        "    f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        ")\n",
        "\n",
        "prompt = (\n",
        "    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) +\n",
        "    prompt_end\n",
        ")\n",
        "\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c457d658",
      "metadata": {
        "id": "c457d658"
      },
      "source": [
        "### Get the Summary using our Trained and Tuned PEFT LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e3b904",
      "metadata": {
        "height": 200,
        "id": "69e3b904",
        "outputId": "91cc76d6-d274-46ba-e411-3c67a151c4d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "The Berlin Wall was built by the communist government of East Germany between the two halves of Berlin. It was to make it hard for American spies to cross, and people decided to tear down the wall. On 13 August 1961, many people were shot dead by East German soldiers when they tried to escape the GDR, according to the SED. The wall divided the city until 1989 when the east German government revoked the law that guaranteed the right to rule the Eastern European Union.\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "peft_model_outputs = peft_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=GenerationConfig(max_new_tokens=3000, temperature=0.5, top_p=1, frequency_penalty=1, presence_penalty=1.5,no_repeat_ngram_size=2,length_penalty=2.0,num_beams=5)\n",
        ")\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print('-' * 80)\n",
        "print(peft_model_text_output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}